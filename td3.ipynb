{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "\n",
    "# Import your custom pendulum environment\n",
    "from quad_copter import QuadcopterEnv\n",
    "\n",
    "# Gymnasium wrapper for your custom environment\n",
    "class GymQuadcopterEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env = QuadcopterEnv()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Action space: single continuous action (force)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-10.0, high=10.0, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        high = np.array([2.4, np.inf, np.pi + 0.2, np.inf], dtype=np.float32)\n",
    "        low = np.array([-2.4, -np.inf, np.pi - 0.2, -np.inf], dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        obs = self.env.reset()\n",
    "        return obs.astype(np.float32), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Extract force from action array\n",
    "        force = float(action[0])\n",
    "        obs, reward, done, info = self.env.step(force)\n",
    "        return obs.astype(np.float32), float(reward), done, False, info\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "# logging\n",
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# create custom environment function\n",
    "def make_env():\n",
    "    env = GymQuadcopterEnv()\n",
    "    env = Monitor(env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "    return env\n",
    "\n",
    "# Create training environment\n",
    "train_env = DummyVecEnv([make_env])\n",
    "\n",
    "n_actions = train_env.action_space.shape[0]\n",
    "\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "# Create TD3 agent\n",
    "model = TD3(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    action_noise=action_noise,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    learning_rate=0.001,\n",
    "    buffer_size=100000,\n",
    "    batch_size=100,\n",
    "    learning_starts=1000,\n",
    "    policy_delay=2,\n",
    "    target_policy_noise=0.2,\n",
    "    target_noise_clip=0.5,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.learn(total_timesteps=50000)\n",
    "model.save(\"td3_inverted_pendulum\")\n",
    "print(\"Training completed and model saved!\")\n",
    "\n",
    "train_env.close()\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating trained agent...\")\n",
    "eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "# Use stable-baselines3's built-in evaluation function\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, \n",
    "    eval_env, \n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "print(f\"Mean evaluation reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Additional detailed evaluation\n",
    "obs = eval_env.reset()\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(5):\n",
    "    obs = eval_env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = eval_env.step(action)\n",
    "        total_reward += reward[0]\n",
    "        steps += 1\n",
    "        \n",
    "        if done[0]:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    print(f\"Episode {episode + 1}: Reward = {total_reward:.2f}, Steps = {steps}\")\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "# Summary statistics and logging\n",
    "print(f\"\\nEvaluation Summary:\")\n",
    "print(f\"Average Reward: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Std Reward: {np.std(episode_rewards):.2f}\")\n",
    "print(f\"Min Reward: {np.min(episode_rewards):.2f}\")\n",
    "print(f\"Max Reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"Average Episode Length: {np.mean(episode_lengths):.1f}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    monitor_data = pd.read_csv(os.path.join(log_dir, \"monitor.csv\"), skiprows=1)\n",
    "    if len(monitor_data) > 0:\n",
    "        print(f\"\\nTraining Statistics:\")\n",
    "        print(f\"Training Episodes: {len(monitor_data)}\")\n",
    "        print(f\"Average Training Reward: {monitor_data['r'].mean():.2f}\")\n",
    "        print(f\"Final Training Reward: {monitor_data['r'].iloc[-1]:.2f}\")\n",
    "        print(f\"Best Training Reward: {monitor_data['r'].max():.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load training statistics: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
